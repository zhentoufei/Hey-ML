##### 前言

​        在实习的时候做预料的规则的主成份分析，用到了PCA，但是只是基于[sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)做的分析，并没有了解其具体的细节，在这里总结一下其原理和自己的理解。

##### 内积

​        定义：

​                                $$(a_1,a_2,...,a_n)^T·(b_1,b_2,...,b_n)=a_1b_1+a_2b_2+...+a_nb_n$$

​        从定义来看的话，内积实质上是把两个向量映射成了一个实数，但是其意义并不明显

​        好吧，来看看从几何意义上怎么去理解吧。假设$$A$$和$$B$$是两个$$n$$维度的向量，在一个$$n$$维度的空间中，这两个向量就是从头指向尾部的有向线段嘛，为了好理解，我们在二位上来理解好吧，那么$$A=(x_1, y_1), B=(x_2,y_2)$$，在二位坐标系中可以这么画：

![img](http://pic1.zhimg.com/8d64151ceed0eed4d4708d8d9e6374dc_b.png)

​                                                                        *图片来自[知乎](https://www.zhihu.com/search?q=PCA&type=content)*

​        现在从$$A$$点向$$B$$所在直线引出一条垂线。那么垂点就是$$A$$在$$B$$上的投影；假设$$A$$与$$B$$的夹角是$$\alpha$$，那么投影的适量长度是$$|A|cos(\alpha)$$

​        那么，我们现在看下面的这个式子：

​                                                                         $$A·B=|A||B|cos(\alpha)$$

​        就可以理解成$$A$$与$$B$$的内积等于$$A$$向$$B$$的投影长度乘以$$B$$的模，ok，有没有一些似曾相识的感觉？什么？没有，那么我们再进一步来看看，如果让$$|B|=1$$，那么是不是会有：

​                                                                                $$A·B=|A|cos(\alpha)$$

​        也就是说，在向量$$B$$的模为1，则$$A$$与$$B$$的内积值等于$$A$$向$$B$$所在直线投影的矢量长度-->这就是对适量内积的理解啦（其实我们在二维坐标系中的坐标不就是这种向$$(1,0)，(0,1)$$上投影得到的嘛）



##### PCA降维（(Principal Component Analysis）

​        **为什么要降维？**

​        （1）多重共线性--预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯                                           

​        （2）高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。

​        （3）过多的变量会妨碍查找规律的建立

​        （4）仅在变量层面上分析可能会忽略变量之间的潜在联系

​        **PCA降维问题的优化目标：**将一组$$N$$维度向量降为$$K$$维度($$0<K<N$$)，其目标是选$$K$$个单位正交基，使得原来的数据变换变换到这组基(一个新的坐标空间)后，各个字段两两之间协方差为0，而且字段的方差尽可能大（在正交的约束下，去最大的K个方差）

​        首先说一说为**什么要方差大**：尽量保证信息不丢失，如果信息的离散程度越小，那么投影到低纬空间的时候，会存在数据重合的现象，那么大量的数据重合，是我们不想要的，会造成有用信息丢失

​        再来说一说**为什么要协方差为0**：

​        我们先看看协方差在干嘛：[协方差](https://en.wikipedia.org/wiki/Covariance)，点开有毒啊，

​        哈哈，其实协方差就是衡量两个变量的相关程度的，在PCA中，由于要做0均值处理(0均值处理实在干嘛？就是为了方便后续的处理，毕竟整体移动数据在空间中的位置不会给数据特征带来影响啊)，协方差的表达式可以如下表达

​                                                                                    $$Cov(a,b)=\frac{1}{m}\sum_{i=1}^{m} a_ib_i$$

​        其中，$$m$$是样本的个数，$$a_i，b_i$$是数据集中的某两个属性。当字段均值为0的情况下，两个字段的协方差可以表示为其内积除以元素数目$$m$$，如果两个属性完全独立，那么协方差为0。那么，当我们确定了第一个基之后，要选择第二个基变量的时候，就尽量希望选择一个正交方向的基。

​        **话是这么说的，但是怎么做呢？**

​        先看一个东西：

​                                              $$X=\bigg[\begin{matrix} a_1&  a_2&  ... & a_m \\ b_1&  b_2&  ... & b_m \end{matrix}\bigg]$$

然后再看下面的变换：

​                                        $$\frac{1}{m}XX^T=\bigg[\begin{matrix}  \frac{1}{m}\sum_{i=1}^{m}a_i^2&\frac{1}{m}\sum_{i=1}^{m}a_ib_i     \\  \frac{1}{m}\sum_{i=1}^{m}a_ib_i & \frac{1}{m}\sum_{i=1}^{m}b_i^2         \end{matrix}\bigg]$$

​        很容易看出来，这个矩阵对角线上的两个元素分别是两个字段的方差，其他元素是元素的协方差。一般画，设我们有$$m$$个$$n$$纬的数据记录，将其按列排成$$n$$乘$$m$$的矩阵$$X$$，设$$C=\frac{1}{m}XX^T$$，则$$C$$是一个对称阵，其对角线分别是各个字段的方差，第$$i$$行$$j$$列和$$j$$行$$i$$列元素相同，表示$$i$$和$$j$$的两个字段的协方差

​        假设，原始数据矩阵对应的协方差矩阵是$$C$$，而$$P$$是一组基(一个空间变换)按行组成的矩阵，设$$Y=PX$$，则$$Y$$为$$X$$对$$P$$做基变换之后的数据。折$$Y$$的协方差矩阵为$$D$$，可以有以下的推导：
​                                                        $$D=\frac{1}{m}YY^T$$

​                                                            $$=\frac{1}{m}(PX)(PX)^T$$

​                                                            $$=\frac{1}{m}PXX^TP^T$$

​                                                            $$=(\frac{1}{m}XX^T)P^T$$

​                                                            $$=PCP^T$$

​        注意哦，回忆一下之前我们的想法：想找一个方差最大，协方差为0的矩阵$$D$$，那么我们就要对矩阵$$C$$做对角化了。怎么对角化？看这个[资料](http://blog.csdn.net/u011484045/article/details/44724369)。

​        好了，到这里，那么我们的问题就基本分析完毕了，只要找到对角矩阵的对角线的几个最大值以及对应特征向量就行了

​        **[到底多少特征是合适的呢？](http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)**（转载）

​        我们该如何选择 $$k$$，即保留多少个PCA主成分？对于高维数据来说，如果$$k$$过大，数据压缩率不高，在极限情况$$k=n$$ 时，等于是在使用原始数据（只是旋转投射到了不同的基）；相反地，如果$$k$$过小，那数据的近似误差太太。

​       决定$$k$$值时，我们通常会考虑不同$$k$$值可保留的方差百分比。具体来说，如果$$k=n$$ ，那么我们得到的是对数据的完美近似，也就是保留了100%的方差，即原始数据的所有变化都被保留下来；相反，如果$$k=0$$ ，那等于是使用零向量来逼近输入数据，也就是只有0%的方差被保留下来。

​       一般而言，设$$\lambda_1,\lambda_2,...,\lambda_n$$ 表示$$\sum$$ 的特征值（按由大到小顺序排列），使得$$\lambda_j$$为对应于特征向量$$u_j$$ 的特征值。那么如果我们保留前$$k$$个成分，则保留的方差百分比可计算为：

​                                                               $$\frac{\sum_{j=1}^{k} \lambda_j}{\sum_{j=1}^{n} \lambda_j}$$


​       在上面简单的二维实验中，$$\lambda_1=7.29，\lambda_2=0.69$$ 。因此，如果保留$$k=1$$ 个主成分，等于我们保留了  $$7.29/(7.29+0.69)=0.913$$，即91.3%的方差。

​       对保留方差的百分比进行更正式的定义已超出了本教程的范围，但很容易证明，$$\lambda_j=\sum_{i=1}^{m} x_{rot,j}^2$$ 。因此，如果 $$ \lambda_j\approx 0$$ ，则说明$$x_{rot,j}$$ 也就基本上接近于0，所以用0来近似它并不会产生多大损失。这也解释了为什么要保留前面的主成分（对应的 值较大$$\lambda_j$$）而不是末尾的那些。 这些前面的主成分 变化性$$x_{rot,j}$$更大，取值也更大，如果将其设为0势必引入较大的近似误差。

​       以处理图像数据为例，一个惯常的经验法则是选择 $$k$$ 以保留99%的方差，换句话说，我们选取满足以下条件的最小$$k$$值：

​                                                               $$\frac{\sum_{j=1}^{k} \lambda_j}{\sum_{j=1}^{n} \lambda_j} \ge0.99$$

​       对其它应用，如不介意引入稍大的误差，有时也保留90-98%的方差范围。若向他人介绍PCA算法详情，告诉他们你选择的 $$k$$保留了95%的方差，比告诉他们你保留了前120个（或任意某个数字）主成分更好理解。

​        **总结一下流程：**

​        设数据集样本数目是$$m​$$，样本特征是$$n​$$维

​        （1）将原始数据组成矩阵$$X_{n\times m}$$

​        （2）将$$X$$的每一行（特征）零均值化

​        （3）求出协方差矩阵$$C=\frac{1}{m}XX^T$$

​        （4）求出协方差矩阵的特征值及对应的特征向量

​        （5）特征向量从大到小排列，找到特征值对应的特征向量，取前$$k$$行组成矩阵$$P$$

​        （6）$$Y=PX$$即为降维到$$k$$维特征后的数据



##### 重要的是要知道其应用在哪里

​        总体上来说，PCA做的事情分为：**1.找到方差最大的特征方向；2.各个正交方向的数据没有相关性**

​        **优点**：

​        （1）以方差衡量信息的无监督学习，不受样本标签限制

​        （2）各主成分之间正交，可消除原始数据成分间的相互影响

​        （3）可减少指标选择的工作量

​        （4）计算方法简单，易于在计算机上实现

​        **缺点**：

​        （1）主成分解释其含义往往具有一定的模糊性，不如原始样本完整，可能有一定的信息丢失

​        （2）贡献率小的主成分往往可能含有对样本差异的重要信息（异常检测）

​        （3）特征值矩阵的正交向量空间是否唯一有待讨论

​        （4）当存在gross corruptions（污染，遮挡）时，PCA不能很好的抓住数据的真实子空间结构，因此效果比较差，特别是遮挡幅值较大时，效果更差

​        **场景**：

​        总的来说，PCA可以很好的解除现行相关，但是对于高阶相关性就没有办法，当然了，对于高阶相关性的我们可以使用伟大的Kernel函数，将非线性转换为线性相关，相关的可以看看[这里](http://blog.csdn.net/wolenski/article/details/7961703)。另外，一定要注意，在使用PCA的时候，数据特征一定要分布在正交方向上，如果非正交方向存在几个方差较大的，那么PCA的效果就不太好了        



------

**参考**

1. [【**机器学习算法实现】主成分分析(PCA)——基于python+numpy](http://blog.csdn.net/u012162613/article/details/42177327)
2. [主成份分析](http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)
3. [CS229 Machine Learning Autumn 2016](http://cs229.stanford.edu/)
4. [PCA数学原理及优缺点](http://blog.sina.com.cn/s/blog_7423cd260102we2t.html)
5. [各种算法的优缺点](http://www.cnblogs.com/Jerry-PR/articles/5411557.html)