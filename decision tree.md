决策树（分类，回归方法）的步骤：特征选择，决策树的生成，决策树的修剪

决策树的本质：从训练集中归纳出一组分类规则。能对训练数据进行正确分类的的决策树可能有多个，也可能一个都没有。，我们需要的是一个与训练集合矛盾较小的决策树，同时具有很好的泛化能力，从另外的一个角度看，决策树的学习是有训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅要对训练数据有很好的拟合，而且对未知数据有很好的预测。

#### 1. 一些基本的定义

**决策树的损失函数**：正则化的极大似然函数

**决策树的学习策略**：以损失函数为目标函数的最小化

**决策树的学习算法采用**：启发式算法，（由于在所有的决策树中选择最优的决策树是NP完全问题，才使用这个方法，近似求解这个问题，得到的是次最优的）

**特征选择的准则：**信息增益或者信息增益比

熵越大不确定性就越大：

熵：

$$H(p)=-\sum_{i=1}^{n} {p_i}logp_i​$$ 

$$P(X=x_i)=p_i，    i=1,2,...,n$$，

条件熵$$H(Y|X)$$表示在已知随机变量$$X$$的条件下随机变量$$Y$$的不确定性。随机变量$$X$$给定的条件下随机变量$$Y$$的条件熵$$H(Y|X)$$，定义为$$X$$给定条件下$$Y$$的条件概率分布的熵对$$X$$的数学期望：

$$H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_i) ,   其中 p_i=P(X=x_i), i=1,2,3...$$

**信息增益：**表示得知特征X的信息而使得类$$Y$$的信息的不确定性减少的程度

**定义（信息增益）：**特征A对训练数据集$$D$$的信息增益$$g(D,A)$$，定义为集合$$D$$的经验熵$$H(D，A)$$，定义为集合$$D$$的经验熵$$H(D)$$与特征$$A$$给定条件下$$D$$的经验条件熵$$H(D|A)$$之差，即：

$$g(D,A)=H(D)-H(D|A)$$

一般的，熵$$H(Y)$$与条件熵$$H(Y|X)$$之差称为互信息（信息增益）。那么决策树中的信息增益的等价于训练数据集中类别与特征的互信息。

**信息增益准则的特征选择方法：**选择信息增益最大的特征

**信息增益比：**以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这个问题进行矫正，这是特征选择的另外一个准则

**定义（信息增益比）：**特征A对训练数据集D的嘻嘻增益比$$g_R(D,A)$$定义为其信息增益$$g(D,A)$$与训练数据集D关于特征A的值的熵$$H_A(D)$$之比，即：

$$g_R=g(D,A)/H_A(D)$$

其中，$$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$，n是特征A的取值个数

 

##### [信息熵 条件熵 信息增益 信息增益比 GINI系数](http://blog.csdn.net/bitcarmanlee/article/details/51488204)

#### 2. 两个经典决策树：ID3, C4.5

**ID3算法：**信息增益准则选择特征（注意采用信息增益准则的不足）

**ID3生成步骤：**...

**C4.5算法：**采用信息增益比选择特征

**C4.5生成步骤：**...

#### 3. 决策树的剪枝

**原因：**过拟合

**剪枝原则：**最小化决策树整体的损失函数(cost function)或者代价函数(loss function)

假设树$$T$$的叶结点个数为$$|T|$$，$$t$$是树$$T$$的叶结点，该叶节点有$$N_t$$个样本点，其中$$k$$类的样本节点有$$N_{tk}$$个，k=1，2，3...K，$$H_t(T)$$为叶节点$$t$$上的经验熵，$$\alpha\geq0$$为参数，则决策树学习的损失函数可以定义为：

$$C_a(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$

其中，经验熵为

$$H_t(T)$=-\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$

在损失函数中，将上式中的第一项记作：$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}$$

这时：$$C_\alpha(T)=C(T)+\alpha|T|$$

在这个式子中，$$C(T)$$表示模型对训练数据的误差（模型对数据拟合程度的度量），$$|T|$$表示模型复杂程度，参数$$\alpha\geq0$$控制两者之间的影响程度。可以理解，较大的$$\alpha$$促使选择更简单的模型，反之更复杂的模型（但是这样会过拟合哦）

**决策树的生成和决策树的剪枝之间的关系**

在决策树的生成的过程中，只考虑了减小上面式子中的第一项，但是，剪枝的过程中，通过优化损失函数还考虑了减小模型的复杂程度；决策树的生成是局部模型的生成，而剪枝的过程是学习整体的模型

我们也可以从$$C_a(T)$$的表达式中发现，对于这个问题，我们损失函数的极小化等价于正则化的极大似然估计

**剪枝过程的基本思路**：如果在剪枝之后的整体的损失函数会变小，那么我们可以将这个树枝剪掉了。



**CART算法：**分类与回归树，可以同时用作分类和回归





**缺失值的处理：**[决策树是如何处理不完整数据的？](https://www.zhihu.com/question/34867991?sort=created)



**剪枝：**



