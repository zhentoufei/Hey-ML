决策树（分类，回归方法）的步骤：特征选择，决策树的生成，决策树的修剪

决策树的本质：从训练集中归纳出一组分类规则。能对训练数据进行正确分类的的决策树可能有多个，也可能一个都没有。，我们需要的是一个与训练集合矛盾较小的决策树，同时具有很好的泛化能力，从另外的一个角度看，决策树的学习是有训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅要对训练数据有很好的拟合，而且对未知数据有很好的预测。

#### 1. 划分选择（一些定义）

​        **(1)决策树的损失函数**：正则化的极大似然函数

​        **(2)决策树的学习策略**：以损失函数为目标函数的最小化

​        **(3)决策树的学习算法采用**：启发式算法，（由于在所有的决策树中选择最优的决策树是NP完全问题，才使用这个方法，近似求解这个问题，得到的是次最优的）

​        **(4)特征选择的准则：**信息增益，信息增益比，基尼指数



​	**1.熵越大不确定性就越大**：

​	              $$H(p)=-\sum_{i=1}^{n} {p_i}logp_i $$   ，   $$P(X=x_i)=p_i，    i=1,2,...,n$$，

​	**2.条件熵**$$H(Y|X)$$

​       表示在已知随机变量$$X$$的条件下随机变量$$Y$$的不确定性。随机变量$$X$$给定的条件下随机变量$$Y$$的条件熵$$H(Y|X)$$，定义为$$X$$给定条件下$$Y$$的条件概率分布的熵对$$X$$的数学期望：

​			$$H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_i) ,   其中 p_i=P(X=x_i), i=1,2,3...$$

​	**3.信息增益：**

​       表示得知特征X的信息而使得类$$Y$$的信息的不确定性减少的程度

​	**定义（信息增益）：**特征A对训练数据集$$D$$的信息增益$$g(D,A)$$，定义为集合$$D$$的经验熵$$H(D，A)$$，定义为集合$$D$$的经验熵$$H(D)$$与特征$$A$$给定条件下$$D$$的经验条件熵$$H(D|A)$$之差，即：

​							$$g(D,A)=H(D)-H(D|A)	$$

​	一般的，熵$$H(Y)$$与条件熵$$H(Y|X)$$之差称为互信息（信息增益）。那么决策树中的信息增益的等价于训练数据集中类别与特征的互信息。

​	**4.信息增益比：**

​       以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这个问题进行矫正，这是特征选择的另外一个准则

​	**定义（信息增益比）：**

​       特征A对训练数据集D的嘻嘻增益比$$g_R(D,A)$$定义为其信息增益$$g(D,A)$$与训练数据集D关于特征A的值的熵$$H_A(D)$$之比，即：

​                                                 $$g_R=g(D,A)/H_A(D)$$

其中，$$H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$，n是特征A的取值个数

​	 **5.定义（基尼指数）：**

​       在分类问题中，假设有K个类，样本点属于第k类的概率为$$p_k$$，则概率分布的基尼指数定义为：

​                                        	$$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^{2}$$

特别的，对于二分类问题，若样本点属于第一个类的概率是$$p$$，则概率分布的基尼指数为

​                                                   	$$Gini(p)=2p(1-p)$$



​	**6.对于给定的样本集合$$D$$**，其基尼指数为

​                                                  	$$Gini(D) = 1 - \sum_{k=1}^k (\frac{|C_k|}{|D|})^{2}$$

在这里，$$C_k$$是$$D$$中属于第$$k$$类的样本子集，$$K$$是类的个数



​	7.**在特征$$A$$的条件下，集合$$D$$的基尼指数**如果样本集合$$D$$根据特征$$A$$是否去某一可能值$$\alpha$$被分割成$$D_1$$ ，$$D_2$$两部分，也就是说：

​                                               $$D_1={(x,y) \subset D|A(x)=\alpha},， D_2=D-D_1$$

则**在特征$$A$$的条件下，集合$$D$$的基尼指数定义为**：

​                                        $$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$

​       	**$$Gini(D)$$和$$Gini(D,A)$$区别：**

​       	$$Gini(D)$$表示集合$$D$$的不确定性

​       	$$Gini(D,A)$$表示进过$$A=\alpha$$分割后，集合D的不确定性

​	基尼指数越大，样本集合的不确定性越大，这一点和熵的定义很类似，在CART中使用的是基尼指数



#### 2.剪枝处理：

​	(1)为啥要剪枝？处理过拟合问题

​	(2)遵循的准则：剪枝后的准确率要比剪枝之前的验证集精确度高

​	**原因：**过拟合

​	**剪枝原则：**最小化决策树整体的损失函数(cost function)或者代价函数(loss function)

​	假设树$$T$$的叶结点个数为$$|T|$$，$$t$$是树$$T$$的叶结点，该叶节点有$$N_t$$个样本点，其中$$k$$类的样本节点有$$N_{tk}$$个，k=1，2，3...K，$$H_t(T)$$为叶节点$$t$$上的经验熵，$$\alpha\geq0$$为参数，则决策树学习的损失函数可以定义为：

​				$$C_a(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$

其中，经验熵为

​				$$H_t(T)$=-\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$

​	在损失函数中，将上式中的第一项记作：$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}$$

​	这时：$$C_\alpha(T)=C(T)+\alpha|T|$$

​	在这个式子中，$$C(T)$$表示模型对训练数据的误差（模型对数据拟合程度的度量），$$|T|$$表示模型复杂程度，参数$$\alpha\geq0$$控制两者之间的影响程度。可以理解，较大的$$\alpha$$促使选择更简单的模型，反之更复杂的模型（但是这样会过拟合哦）

​	**决策树的生成和决策树的剪枝之间的关系**

​	在决策树的生成的过程中，只考虑了减小上面式子中的第一项，但是，剪枝的过程中，通过优化损失函数还考虑了减小模型的复杂程度；决策树的生成是局部模型的生成，而剪枝的过程是学习整体的模型

我们也可以从$$C_a(T)$$的表达式中发现，对于这个问题，我们损失函数的极小化等价于正则化的极大似然估计

​	**剪枝过程的基本思路**：如果在剪枝之后的整体的损失函数会变小，那么我们可以将这个树枝剪掉了。

​	**预剪枝：**

​	**优势：**该方法使得决策树的很多分支都没有“展开”，这不仅降低了过拟合风险，还显著减少了决策树的训练时间开销和测试时间开销。

​	**劣势：** 1.有一些分支的当前划分虽然不能提升泛化能力、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能的显著提高

​		     2.预剪枝基于“贪心”本质禁止这些分支展开，可能会导致欠拟合

​	**后剪枝：**

​	相比预剪枝，后剪枝保留了更多的分支

​	**优势：**更小的欠拟合风险，泛化能力强于预剪枝

​	**劣势：**后剪枝要在决策树完全生成后进行，并且要自底向上地对树中的所有非叶子节点逐一考察，需要比前者更多的训练时间

#### 3.连续值

​	就是对某一个连续属性的属性值进行排序，然后去相邻两个属性值的平均值作为分裂的点，然后在分裂点左右分别计算相应的划分准则

​	也就是说把区间$$[a^{i}, a^{i+1})$$的中位点$$\frac{(a^{i}+a^{i+1})}{2}$$作为候选划分点，然后我们就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分

对信息增益稍加改造：

​                        $$Gain(D,a)=\max_{t\subset T_a}Gain(D,a,t)=\max_{t\subset T_a}H(D)-\sum_{\lambda\subset\{-,+\}}\frac{|D_t^\lambda|}{|D|}H(D_t^\lambda)$$

其中，$$Gain(D,a,t)$$是样本集$$D$$基于划分点$$t$$二分之后的信息增益。于是，我们就可以选择使$$Gain(D,a,t)$$最大的划分点

​	详细的见[决策树是如何处理不完整数据的？](https://www.zhihu.com/question/34867991?sort=created)

#### 4缺失值

在面对缺失值的时候面临的两个问题：

(1)如何在属性值缺失的情况下进行划分属性选择？

​	基于信息增益的推广：

​                  	$$Gain(D,a)=\rho*Gain(\tilde{D})=\rho*(H(\tilde{D}-\sum_{v=1}^{V}\tilde{r_v}H(\tilde{D_{}^{v}})))$$

​	其中，训练集合$$D$$表示$$D$$中在属性$$\alpha$$上没有缺失值的样本子集。$$\tilde{r_v}$$表示无缺失值样本中，$$\alpha$$的属性

(2)给定划分属性，若该样本在该属性上的值缺失，如何对样本进行划分？

​	那么就对这个样本放在左边的树试一试，看看计算出来的增益增比，增益越大，那么就选择这个



#### 5. 经典决策树：ID3, C4.5, CART

**ID3算法：**信息增益准则选择特征，选择**信息增益最大**的（注意采用信息增益准则的不足）

**ID3生成步骤：**...

**C4.5算法：**采用**信息增益比最大**选择特征

**C4.5生成步骤：**...

**CART算法：**分类与回归树，可以同时用作分类和回归，选择基尼系数最小的特征及其对应的切分点作为最优特征和最优切分点

**CART生成步骤:**   ...



**6.关于sklearn中的使用**

[sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)

[Decision Trees](http://scikit-learn.org/stable/modules/tree.html)

------

**参考**

1. [信息熵 条件熵 信息增益 信息增益比 GINI系数](http://blog.csdn.net/bitcarmanlee/article/details/51488204)
2. 统计学方法， 李航
3. 机器学习， 周志华
4. [sklearn中的cart分类树](http://d0evi1.com/sklearn/cart/)
5. [1.10. Decision Trees](http://scikit-learn.org/stable/modules/tree.html)