##### 1.SVD简介

​        svd是一种矩阵分解方法，叫奇异值分解

##### 2.LSI对SVD的使用

​        LSI 对 SVD 做了一点改变，就是对 S 的 r 个对角线元素进行了排序，并只保留前 k 个值 ( k < r )， 后 r - k 个置零。

​        LSI 是通过舍弃不重要的特征向量来达到降维效果的，而又由于特征向量是根据矩阵运算得出的，因此** LSI 在降维的过程中不仅丢失了信息，而且还改变了信息。降维后的数据集仅仅是对原数据集的一种近似而非等价形式。且降维幅度越大，与原信息的偏离就越大。

##### 3.LSI的适用性

​        LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。在降维这块土地上还有另一位辛勤的耕耘者那就是TFIDF，TFIDF通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，这里只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF

​        不过必须提醒的是，**无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。**然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）

##### 4. 单词相关度计算

​        LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。

##### 5.聚类

​        直接用 LSI 聚类的情景我还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。

​    对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。

##### 数值稳定性

通过SVD可以得到PCA相同的结果，但是SVD通常比直接使用PCA更稳定。因为PCA需要计算$$X^TX$$的值，对于某些矩阵，求协方差时很可能会丢失一些精度。例如Lauchli矩阵