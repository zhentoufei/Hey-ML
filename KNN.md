##### 1.KNN（k-nearest neighbor classification）算法简介

​        在[模式识别](https://zh.wikipedia.org/wiki/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB)领域中，**最近邻居法**（**KNN**算法，又译**K-近邻算法**）是一种用于[分类](https://zh.wikipedia.org/wiki/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)和[回归](https://zh.wikipedia.org/wiki/%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90)的[非参数统计](https://zh.wikipedia.org/wiki/%E7%84%A1%E6%AF%8D%E6%95%B8%E7%B5%B1%E8%A8%88)方法。在这两种情况下，输入包含[特征空间](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4)中的$$k$$个最接近的训练样本。

- 在k-NN*分类*中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，*k*个最近邻居（*k*为正[整数](https://zh.wikipedia.org/wiki/%E6%95%B4%E6%95%B0)，通常较小）中最常见的分类决定了赋予该对象的类别。若*k* = 1，则该对象的类别直接由最近的一个节点赋予
- 在*k-NN回归*中，输出是该对象的属性值。该值是其*k*个最近邻居的值的平均值



##### 2.KNN算法主要步骤

​        kNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。

计算步骤如下：

​    1）算距离：给定测试对象，计算它与训练集中的每个对象的距离

​    2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻

​    3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类

关于**距离度量**猛抽[这里](http://blog.csdn.net/u011630575/article/details/52164688)，或者[这里](http://blog.csdn.net/pipisorry/article/details/45651315)

关于**类别判定**：1.	投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。 

​                           2.加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平	                 

​                              方的倒数）



##### **3.过拟合和欠拟合：**

​        **核心问题：**K值的选择对K近邻算法的结果会产生重大影响。

​        **K值较小(过拟合)：**就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小， K值的减小就意味着整体模型变得复杂，容易发生过拟合；

​        **K值较大(欠拟合)：**就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。k很大，那么可以减少干扰数据的影响，但是此时就导致了系统性偏差（K值太小会造成过度拟合），比如如果取k为总的训练数据数，那么每次投票肯定都是训练数据中多的类别胜利。显然训练数据的系统性偏差会影响结果。



##### 4.KNN优缺点

​        **优点:**

​        算法简单，易于实现，不需要参数估计，不需要事先训练

​        适合对稀有事件进行分类

​        适合多分类问题，有些时候的表现性能逼SVM还要好

​        对异常值不敏感

​        **缺点:**

​        属于懒惰算法，kNN不用事先训练，而是在输入待分类样本时才开始运行，这一特点导致kNN计算量特别大，而且训练样本必须存储在本地，内存开销也特别大          



##### 5.适用场景：

客户流失预测、欺诈侦测等（更适合于稀有事件的分类问题）



##### 6.关于KNN中K的取值

​        通常**不大于20**。——《机器学习实战》

​        通常情况下，选择不同的k 会使得我们的算法的表现有好有坏，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。**k通常会在3～10直接取值**，或者是k等于**训练数据的平方根**。比如15个数据，可能会取k=4。在实际中，我们应该通过**交叉验证**的办法来确定k值。



##### **7.如何使用[sklearn中的knn](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)**

```python
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y) 
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[ 0.66666667  0.33333333]]
```



