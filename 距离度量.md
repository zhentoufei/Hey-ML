**前言**

​        最近重新看了KNN等聚类算法，感觉对距离度量记忆的有些混乱，在这里把常用的几个整理出来

##### 0. 关于距离度量

​        在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。常见的是数据分析中的关联分析，数据挖掘中得到分类和聚类算法，如KNN和K-Means



##### 1. 不同度量方式的应用场景

空间：欧氏距离

路径：曼哈顿距离

国际象棋国王：切比雪夫距离，这三种可以统一为闵可夫斯基距离

加权：标准化的欧氏距离

排除量纲和依存：马氏距离

向量差距：夹角余弦

编码差别：汉明距离

集合相似度：Jaccard相似系数

相关性：相关系数与相关距离



##### 2. 距离度量公理



##### 3. 文本相似度度量方法

![img](http://www.zmonster.me/assets/img/similarity_survey.png)

*图片来自[*[距离和相似度度量方法](http://blog.csdn.net/pipisorry/article/details/45651315)*]*

关于文本相似性度量的理论研究的比较少，详细的可以猛戳[文本相似度量方法(1): 概览](http://www.zmonster.me/2015/11/15/text_similarity_survey.html)



##### 4.欧式距离度量

(1)Manhattan距离(L1范数)：$$D=\sum_{i=1}^{n}|x_i-y_i|$$

(2)欧氏距离(L2范数)：$$D=(\sum_{i=1}^{n}|x_i-y_i|^2)^{1/2}$$

(3)Lp范数(Minkowski )：$$D=(\sum_{i=1}^{n}|x_i-y_i|^p)^{1/p}$$

(4)L-$$\infty$$范数(切比雪夫距离)：$$D={\lim_{x \to +\infty}} (\sum_{i=1}^{n}|x_i-y_i|^p)^{1/p}$$



##### 5. 非欧氏距离度量-Jaccard

Jaccard 相似性度量

Jaccard distance = 1 - Jaccard similaity

$$J(A,B)=\frac{|A\bigcap B|}{|A\bigcup B|}$$，分子是集合交集，分母是集合并集



##### 6. 非欧氏距离-Cosine余弦相似度(向量内积)

比较适合高纬度的向量相似度计算：

​        1.两个向量之间的余弦值

​                           $$similarity=cos(\theta)=\frac{A·B}{|A||B|}=\frac{\sum_{i=1}^{n} A_i \times B_i}{\sqrt{\sum_{i=1}^{n} (A_i)^2}\times \sqrt{\sum_{i=1}^{n}(B_i)^2}}$$

​        可以发现上式子的计算结果$$\in [-1, 1]$$，-1意味着两个向量指向的方向正好截然相反，1表示他们的指向是完全相同的，0通常表示它们之间是独立的，而在这之间的值则表示中度的相似性或相异性



**7. 向量内积的相似度**

​                         $$Inner(x,y)=<x,y>=\sum_{i}x_iy_i$$

直观理解：如果$$x_i$$某一维度比较大的时候，$$y_i$$的某一纬度的值也比较大。可以发现向量内积的结果是没有界限的，一种解决办法是初一长度后在求积，这就是雨轩相似度了

​                       $$CosSim(x,y)=\frac{\sum x_iy_i}{\sqrt{\sum{x_i}^2\times \sqrt{\sum{y_i}^2}}}$$

这个方法计算的相似度和向量的幅度值无关，只和方向有关，在文档相似度计算(TF-IDF)和图片相似性计算的时候都有他的身影



**8. 皮尔逊系数**

数值型数据的相关性分析Correlation Analysis (Numerical Data)，从下面这个公式看出，它其实就是将数据归一化（数据减去其对应均值）后进行cosine相似度计算，所以叫centered cosine

​        $$Corr(x,y)=\frac{\sum (x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum{x-\overline x}^2}\sqrt{\sum{y_i-\overline{y}}^2}}=\frac{<x-\overline{x}, y-\overline{y}>}{|x-\overline{x}||y-\overline{y}|}=CosSim(x-\overline x, y-\overline {y})$$

另一种表达方式

​        $$r_{A,B}=\frac{\sum(a_i-\overline A)(b_i-\overline B)}{(n-1)\sigma_a\sigma_b}$$

皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。

相关系数的强弱仅仅看**系数的大小是不够的**。一般来说，**取绝对值**后，0-0.09为没有相关性，0.3-弱，0.1-0.3为弱相关，0.3-0.5为中等相关，0.5-1.0为强相关。

###### 皮尔逊相关系数的适用范围

当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：

1. 两个变量之间是线性关系，都是连续数据。
2. 两个变量的总体是正态分布，或接近正态的单峰分布。
3. 两个变量的观测值是成对的，每对观测值之间相互独立。



**9. 汉明距离**

**汉明距离**（Hamming distance）是指，两个等长字符串(一般适用于bit  vectors)s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。



------

**参考**

1. [距离和相似度度量方法](http://blog.csdn.net/pipisorry/article/details/45651315)

2. [漫谈：机器学习中距离和相似性度量方法](http://www.cnblogs.com/daniel-D/p/3244718.html)

3. [大规模数据相似度计算时，解决数据倾斜的问题的思路之一（分块思想）](http://blog.csdn.net/lilyth_lilyth/article/details/9269693)

   ​